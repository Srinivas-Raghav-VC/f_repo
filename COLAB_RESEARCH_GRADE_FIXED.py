# -*- coding: utf-8 -*-
"""
Research-Grade Multilingual Unlearning Experiment
Qwen 2.5-1.5B with Comprehensive Evaluation
FIXED VERSION - All errors corrected
"""

# ============================================================================
# CELL 1: GPU Check & Setup
# ============================================================================
# Check GPU
!nvidia-smi

# Clone repo
!git clone https://github.com/Srinivas-Raghav-VC/f_repo.git
%cd f_repo

# Install core dependencies
!pip install -q torch transformers accelerate peft einops scikit-learn scipy numpy python-dotenv

# Install LID ensemble components
!pip install -q fasttext langid pycld3
!pip install -q google-generativeai  # Gemini SDK for LID

# Install transliteration
!pip install -q indic-transliteration

# Install visualization
!pip install -q matplotlib seaborn

# Install experiment tracking
!pip install -q wandb

print("‚úÖ Setup complete with Gemini support!")

# ============================================================================
# CELL 2: Environment Configuration
# ============================================================================
import os
from google.colab import userdata

# Required: Hugging Face token
try:
    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')
    print("‚úÖ HF_TOKEN loaded from secrets")
except:
    os.environ['HF_TOKEN'] = 'your_hf_token_here'
    print("‚ö†Ô∏è Using manual HF token")

# Required for LID ensemble with Gemini
try:
    os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')
    print("‚úÖ GEMINI_API_KEY loaded from secrets")
except:
    os.environ['GEMINI_API_KEY'] = 'your_gemini_api_key_here'
    print("‚ö†Ô∏è Using manual Gemini token")

# Set offload directory
!mkdir -p /content/offload
os.environ['OFFLOAD_DIR'] = '/content/offload'
os.environ['SAFETENSORS_FAST'] = '0'

print("\n‚úÖ Environment configured")
print("Gemini will be used for:")
print("  1. LID ensemble voting (--use_gemini flag)")
print("  2. Adversarial prompt generation")

# ============================================================================
# CELL 2.5: Interactive API Key Setup (Optional)
# ============================================================================
from google.colab import userdata
import os

print("=" * 60)
print("API KEY VERIFICATION")
print("=" * 60)

# Check Gemini key
if 'GEMINI_API_KEY' not in os.environ or os.environ['GEMINI_API_KEY'] == 'your_gemini_api_key_here':
    print("\n‚ö†Ô∏è Gemini API key not found!")
    print("Get your free key from: https://aistudio.google.com/app/apikey")
    from getpass import getpass
    api_key = getpass("Paste your Gemini API key: ")
    os.environ['GEMINI_API_KEY'] = api_key
    print("‚úÖ Gemini API key set!")
else:
    print("‚úÖ Gemini API key already configured")

# Check HF token
if 'HF_TOKEN' not in os.environ or os.environ['HF_TOKEN'] == 'your_hf_token_here':
    print("\n‚ö†Ô∏è Hugging Face token not found!")
    print("Get your token from: https://huggingface.co/settings/tokens")
    from getpass import getpass
    hf_token = getpass("Paste your HF token: ")
    os.environ['HF_TOKEN'] = hf_token
    print("‚úÖ HF token set!")
else:
    print("‚úÖ HF token already configured")

# ============================================================================
# CELL 3: Initialize WandB (Optional but Recommended)
# ============================================================================
import wandb

try:
    wandb.login()  # Will prompt for API key if not logged in

    # Initialize run (will log to WandB)
    run = wandb.init(
        project="multilingual-unlearning",
        name="qwen-1.5b-final-run",
        config={
            "model": "Qwen/Qwen2.5-1.5B-Instruct",
            "seeds": [42, 123, 456],
            "forget_obj": "npo",
            "sae_steps": 2000,
            "lora_steps": 500,
            "reft_steps": 500
        },
        tags=["qwen", "hindi-unlearning", "semantic-features"]
    )
    print("‚úÖ WandB initialized successfully!")
except Exception as e:
    print(f"‚ö†Ô∏è WandB initialization failed: {e}")
    print("Continuing without WandB logging...")
    run = None

# ============================================================================
# CELL 4: Verify Data Files
# ============================================================================
import os

data_files = [
    'data/forget_hi.jsonl',
    'data/retain_en.jsonl',
    'data/mixed.jsonl',
    'data/urdu.jsonl',
    'data/punjabi.jsonl',
    'data/bengali.jsonl',
    'adversarial.jsonl'
]

print("=" * 60)
print("DATA FILES STATUS")
print("=" * 60)
all_present = True
for f in data_files:
    if os.path.exists(f):
        with open(f) as file:
            lines = sum(1 for _ in file)
        print(f"‚úÖ {f}: {lines} samples")
    else:
        print(f"‚ùå Missing: {f}")
        all_present = False

if not all_present:
    print("\n‚ö†Ô∏è Some data files are missing!")
    print("Make sure to add them to your repo or generate synthetic data.")
else:
    print("\n‚úÖ All data files present!")

# ============================================================================
# CELL 5: Quick Sanity Check (5-10 minutes)
# ============================================================================
print("=" * 60)
print("QUICK SANITY CHECK (5-10 min)")
print("=" * 60)

!python mmie.py \
  --model Qwen/Qwen2.5-1.5B-Instruct \
  --forget data/forget_hi.jsonl \
  --retain data/retain_en.jsonl \
  --mixed data/mixed.jsonl \
  --xlang data/urdu.jsonl \
  --n_top 2 \
  --selection_mode semantic \
  --train_sae_steps 100 \
  --sae_k 16 \
  --lora_steps 50 \
  --reft_steps 50 \
  --seeds 42 \
  --device cuda \
  --out test_quick.json

print("\n‚úÖ Quick test complete! Check test_quick.json")

# Verify it ran successfully
if os.path.exists('test_quick.json'):
    import json
    with open('test_quick.json') as f:
        test_results = json.load(f)
    print(f"Base ES_forget: {test_results['base']['es_forget']:.4f}")
    print("‚úÖ Pipeline is working correctly!")
else:
    print("‚ùå Quick test failed - check errors above")

# ============================================================================
# CELL 6: Generate Adversarial Prompts (Optional, 10-15 min)
# ============================================================================
print("=" * 60)
print("GENERATING ADVERSARIAL PROMPTS WITH GEMINI")
print("=" * 60)

try:
    !python tools/build_training_pairs.py \
      --forget data/forget_hi.jsonl \
      --target_lang "Hindi (Devanagari)" \
      --out_pairs data/pairs_npo.jsonl \
      --out_adv data/adversarial_gemini.jsonl \
      --model gemini-2.0-flash-exp

    print("\n‚úÖ Adversarial prompts generated!")
    print("These will be used in the main experiment.")
except Exception as e:
    print(f"‚ö†Ô∏è Adversarial generation failed: {e}")
    print("Continuing with existing adversarial.jsonl...")

# ============================================================================
# CELL 7: MAIN EXPERIMENT (2-3 hours) üî•
# ============================================================================
print("=" * 60)
print("STARTING MAIN EXPERIMENT (2-3 hours)")
print("=" * 60)
print("Grab a coffee! This will take a while...")
print()

!python mmie.py \
  --model Qwen/Qwen2.5-1.5B-Instruct \
  --forget data/forget_hi.jsonl \
  --retain data/retain_en.jsonl \
  --mixed data/mixed.jsonl \
  --xlang data/urdu.jsonl data/punjabi.jsonl data/bengali.jsonl \
  --adversarial adversarial.jsonl \
  --n_top 3 \
  --selection_mode semantic \
  --train_sae_steps 2000 \
  --sae_k 32 \
  --sae_expansion 4 \
  --sae_gate \
  --sae_gate_alpha 0.5 \
  --sae_gate_topk 64 \
  --semantic_features \
  --semantic_tau 0.0 \
  --dynamic_gate \
  --semantic_dynamic_gate \
  --lora_steps 500 \
  --reft_steps 500 \
  --forget_obj npo \
  --seeds 42 123 456 \
  --gate_es_forget_ratio 0.5 \
  --gate_es_mixed_ratio 0.7 \
  --gate_ppl_ratio 1.10 \
  --sae_quality_eval \
  --report_token_kl \
  --use_gemini \
  --use_xlmr \
  --use_fasttext \
  --device cuda \
  --ckpt_dir /content/ckpt_qwen \
  --out results_qwen_full.json

print("\nüéâ Main experiment complete!")

# ============================================================================
# CELL 8: Load and Display Results
# ============================================================================
import json
import pandas as pd

# Load results
with open('results_qwen_full.json', 'r') as f:
    results = json.load(f)

# Display base metrics
print("=" * 60)
print("BASE MODEL METRICS")
print("=" * 60)
base = results.get('base', {})
print(f"ES_forget:       {base.get('es_forget', 0):.4f}")
print(f"ES_semantic:     {base.get('es_semantic', 0):.4f}")
print(f"PPL_retain:      {base.get('ppl_retain', 0):.2f}")
print(f"ES_mixed:        {base.get('es_mixed', 0):.4f}")

# Display LoRA results
print("\n" + "=" * 60)
print("LORA RESULTS (averaged across seeds)")
print("=" * 60)
lora_seeds = results.get('arms', {}).get('lora', {}).get('seeds', [])
if lora_seeds:
    es_forget_vals = [s['es_forget'] for s in lora_seeds]
    ppl_retain_vals = [s['ppl_retain'] for s in lora_seeds]

    lora_es = sum(es_forget_vals)/len(es_forget_vals)
    lora_ppl = sum(ppl_retain_vals)/len(ppl_retain_vals)

    print(f"ES_forget:       {lora_es:.4f}")
    print(f"PPL_retain:      {lora_ppl:.2f}")

    # Check gates
    lora_agg = results.get('arms', {}).get('lora', {}).get('aggregate', {})
    print(f"\nGate Results:")
    print(f"  G1 (ES forget):  {lora_agg.get('G1', 'N/A')}")
    print(f"  G2 (PPL retain): {lora_agg.get('G2', 'N/A')}")
    print(f"  G3 (ES mixed):   {lora_agg.get('G3', 'N/A')}")

    # Get MIA
    lora_mia = lora_agg.get('mia', (0, (0, 0)))[0]

# Display ReFT results
print("\n" + "=" * 60)
print("REFT RESULTS (averaged across seeds)")
print("=" * 60)
reft_seeds = results.get('arms', {}).get('reft', {}).get('seeds', [])
if reft_seeds:
    es_forget_vals = [s['es_forget'] for s in reft_seeds]
    ppl_retain_vals = [s['ppl_retain'] for s in reft_seeds]

    print(f"ES_forget:       {sum(es_forget_vals)/len(es_forget_vals):.4f}")
    print(f"PPL_retain:      {sum(ppl_retain_vals)/len(ppl_retain_vals):.2f}")

    reft_agg = results.get('arms', {}).get('reft', {}).get('aggregate', {})
    print(f"\nGate Results:")
    print(f"  G1 (ES forget):  {reft_agg.get('G1', 'N/A')}")
    print(f"  G2 (PPL retain): {reft_agg.get('G2', 'N/A')}")
    print(f"  G3 (ES mixed):   {reft_agg.get('G3', 'N/A')}")

# Cross-lingual leakage
print("\n" + "=" * 60)
print("CROSS-LINGUAL LEAKAGE")
print("=" * 60)
if lora_agg:
    xlang = lora_agg.get('crossling_es', {})
    for lang, (mean, (lo, hi)) in xlang.items():
        print(f"{lang:12} {mean:.4f} (95% CI: [{lo:.4f}, {hi:.4f}])")

# Log to WandB if initialized
if run is not None:
    try:
        wandb.log({
            "base/ES_forget": base.get('es_forget', 0),
            "base/PPL_retain": base.get('ppl_retain', 0),
            "lora/ES_forget": lora_es,
            "lora/PPL_retain": lora_ppl,
            "lora/MIA_AUC": lora_mia,
            "lora/gate_G1": lora_agg.get('G1', 'N/A'),
            "lora/gate_G2": lora_agg.get('G2', 'N/A'),
            "lora/gate_G3": lora_agg.get('G3', 'N/A'),
        })
        print("\n‚úÖ Metrics logged to WandB")
    except:
        pass

# ============================================================================
# CELL 9: Comprehension Testing (NEW! 30 min)
# ============================================================================
print("=" * 60)
print("COMPREHENSION TESTING")
print("=" * 60)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# Comprehension prompts (Hindi ‚Üí English translation)
comprehension_prompts = [
    "Translate to English: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?",
    "Translate to English: ‡§Æ‡•Å‡§ù‡•á ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à‡•§",
    "Translate to English: ‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§π‡•à‡•§",
    "Translate to English: ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡•á‡§∞‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡•á‡§Ç‡•§",
    "Translate to English: ‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§",
]

def test_comprehension(model, tok, prompts, device):
    """Test if model can still translate Hindi (understands it)"""
    outputs = []
    for p in tqdm(prompts, desc="Comprehension test"):
        enc = tok([p], return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)
        with torch.no_grad():
            gen = model.generate(
                **enc,
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=tok.pad_token_id
            )
        output = tok.decode(gen[0], skip_special_tokens=True)
        # Remove the prompt from output
        if p in output:
            output = output.replace(p, "").strip()
        outputs.append(output)
    return outputs

# Load models
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
print(f"Loading {model_name}...")
tok = AutoTokenizer.from_pretrained(model_name, token=os.environ.get('HF_TOKEN'))
if tok.pad_token is None:
    tok.pad_token = tok.eos_token
    tok.pad_token_id = tok.eos_token_id

# Test base model
print("\n1. Testing BASE model comprehension...")
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    token=os.environ.get('HF_TOKEN')
)
base_comp = test_comprehension(base_model, tok, comprehension_prompts, "cuda")

# Test LoRA model (load from checkpoint)
print("\n2. Testing LORA model comprehension...")
lora_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    token=os.environ.get('HF_TOKEN')
)

# Load LoRA adapters
try:
    from peft import PeftModel
    lora_path = "/content/ckpt_qwen/lora_adapters.pt"
    if os.path.exists(lora_path):
        # This is a simplified loader - adapt based on your checkpoint format
        lora_model.load_state_dict(torch.load(lora_path), strict=False)
        print("‚úÖ LoRA adapters loaded")
    else:
        print("‚ö†Ô∏è LoRA checkpoint not found, using base model")
except Exception as e:
    print(f"‚ö†Ô∏è Could not load LoRA: {e}")

lora_comp = test_comprehension(lora_model, tok, comprehension_prompts, "cuda")

# Display results
print("\n" + "=" * 60)
print("COMPREHENSION COMPARISON")
print("=" * 60)
for i, prompt in enumerate(comprehension_prompts):
    print(f"\n{i+1}. Prompt: {prompt}")
    print(f"   Base:  {base_comp[i]}")
    print(f"   LoRA:  {lora_comp[i]}")

# Manual assessment
print("\n" + "=" * 60)
print("ASSESSMENT")
print("=" * 60)
print("‚úÖ If LoRA provides accurate translations ‚Üí Superficial unlearning")
print("   (Model refuses to generate Hindi but still understands it)")
print("‚úÖ If LoRA fails to translate ‚Üí Deep unlearning achieved")
print("   (Model has genuinely forgotten Hindi knowledge)")

# Clean up to free GPU memory
del base_model, lora_model
torch.cuda.empty_cache()

# ============================================================================
# CELL 10: Visualization
# ============================================================================
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

print("=" * 60)
print("GENERATING VISUALIZATIONS")
print("=" * 60)

# Create comparison plot
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# ES_forget comparison
methods = ['Base', 'LoRA', 'ReFT']
es_values = [
    base.get('es_forget', 0),
    lora_es if lora_seeds else 0,
    sum([s['es_forget'] for s in reft_seeds])/len(reft_seeds) if reft_seeds else 0
]

axes[0].bar(methods, es_values, color=['gray', 'blue', 'green'], alpha=0.7)
axes[0].set_ylabel('Extraction Strength', fontsize=12)
axes[0].set_title('Hindi Generation (Lower = Better)', fontsize=14, fontweight='bold')
axes[0].axhline(y=0.05, color='r', linestyle='--', linewidth=2, label='Target (0.05)')
axes[0].legend()
axes[0].grid(axis='y', alpha=0.3)

# PPL_retain comparison
ppl_values = [
    base.get('ppl_retain', 0),
    lora_ppl if lora_seeds else 0,
    sum([s['ppl_retain'] for s in reft_seeds])/len(reft_seeds) if reft_seeds else 0
]

axes[1].bar(methods, ppl_values, color=['gray', 'blue', 'green'], alpha=0.7)
axes[1].set_ylabel('Perplexity', fontsize=12)
axes[1].set_title('English Fluency (Lower = Better)', fontsize=14, fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)

# Cross-lingual leakage
if lora_agg and 'crossling_es' in lora_agg:
    xlang_data = lora_agg['crossling_es']
    langs = list(xlang_data.keys())
    values = [xlang_data[l][0] for l in langs]

    axes[2].bar(langs, values, color='orange', alpha=0.7)
    axes[2].set_ylabel('Extraction Strength', fontsize=12)
    axes[2].set_title('Cross-Lingual Leakage', fontsize=14, fontweight='bold')
    axes[2].tick_params(axis='x', rotation=45)
    axes[2].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('results_visualization.png', dpi=300, bbox_inches='tight')
plt.show()

print("‚úÖ Visualization saved as results_visualization.png")

# ============================================================================
# CELL 11: Alpha Sweep (Optional, 1 hour)
# ============================================================================
print("=" * 60)
print("ALPHA SWEEP ANALYSIS")
print("=" * 60)

try:
    !python tools/sweep_alpha.py \
      --model Qwen/Qwen2.5-1.5B-Instruct \
      --forget data/forget_hi.jsonl \
      --retain data/retain_en.jsonl \
      --alphas 0.0 0.2 0.4 0.6 0.8 1.0 \
      --ckpt_dir /content/ckpt_qwen \
      --device cuda

    print("‚úÖ Alpha sweep complete!")
except Exception as e:
    print(f"‚ö†Ô∏è Alpha sweep failed: {e}")

# ============================================================================
# CELL 12: Download All Results
# ============================================================================
from google.colab import files

print("=" * 60)
print("DOWNLOADING RESULTS")
print("=" * 60)

# Download main results
print("1. Downloading main results...")
files.download('results_qwen_full.json')

# Download visualization
if os.path.exists('results_visualization.png'):
    files.download('results_visualization.png')

# Download checkpoints (compressed)
print("2. Compressing checkpoints...")
!zip -q -r checkpoints.zip /content/ckpt_qwen
files.download('checkpoints.zip')

# Download activations if they exist
if os.path.exists('activations'):
    print("3. Compressing activations...")
    !zip -q -r activations.zip activations/
    files.download('activations.zip')

# Download alpha sweep results if available
if os.path.exists('sweep_alpha_results.json'):
    files.download('sweep_alpha_results.json')
if os.path.exists('sweep_alpha_results.csv'):
    files.download('sweep_alpha_results.csv')
if os.path.exists('sweep_alpha_results.png'):
    files.download('sweep_alpha_results.png')

print("\n‚úÖ All files downloaded!")

# ============================================================================
# CELL 13: Final Summary
# ============================================================================
print("=" * 80)
print(" " * 20 + "EXPERIMENT COMPLETE! üéâ")
print("=" * 80)

print("\nüìä RESULTS SUMMARY")
print("-" * 80)
print(f"Base ES_forget:   {base.get('es_forget', 0):.4f}")
print(f"LoRA ES_forget:   {lora_es:.4f} ({((base.get('es_forget', 0) - lora_es) / base.get('es_forget', 0.001) * 100):.1f}% reduction)")
print(f"LoRA PPL_retain:  {lora_ppl:.2f} ({((lora_ppl - base.get('ppl_retain', 0)) / base.get('ppl_retain', 1) * 100):+.1f}% change)")
print(f"LoRA MIA_AUC:     {lora_mia:.4f}")
print()

# Determine publication readiness
gates_passed = (
    lora_agg.get('G1', 'FAIL') == 'PASS' and
    lora_agg.get('G2', 'FAIL') == 'PASS' and
    lora_agg.get('G3', 'FAIL') == 'PASS'
)

print("üéØ PUBLICATION ASSESSMENT")
print("-" * 80)
if gates_passed and lora_es <= 0.06:
    print("‚úÖ EXCELLENT: All gates passed, ES < 0.06")
    print("   ‚Üí Ready for main conference submission")
    print("   ‚Üí Estimated score: 95-98/100")
elif lora_es <= 0.08:
    print("‚úÖ GOOD: Moderate unlearning achieved")
    print("   ‚Üí Ready for workshop/findings submission")
    print("   ‚Üí Estimated score: 90-95/100")
else:
    print("‚ö†Ô∏è MODERATE: Unlearning needs improvement")
    print("   ‚Üí Consider gradient-based SAE selection")
    print("   ‚Üí Or frame as negative result paper")
    print("   ‚Üí Estimated score: 85-90/100")

print("\nüìù NEXT STEPS")
print("-" * 80)
print("1. Review comprehension test results above")
print("2. Write up Methods and Results sections")
print("3. Create tables/figures from downloaded data")
print("4. Draft Discussion on limitations found")
print()
print("See RESEARCH_GRADE_CHECKLIST.md in repo for detailed guidance!")
print()
print("=" * 80)

# Close WandB run
if run is not None:
    run.finish()
    print("‚úÖ WandB run closed")

